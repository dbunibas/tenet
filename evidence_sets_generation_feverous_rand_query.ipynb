{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "import time\n",
    "import multiprocessing\n",
    "import traceback\n",
    "from database.feverous_db import FeverousDB\n",
    "from utils.wiki_page import WikiPage\n",
    "from pandasql import sqldf\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import copy\n",
    "import json\n",
    "from time import time\n",
    "from wcwidth import wcswidth\n",
    "import random\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Path to the main folder of the dataset\n",
    "base_path = '/homes/bussotti/feverous_work/feverousdata'\n",
    "db_path=base_path+'/shareddata/filtereddb_st_2.db'\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "os.chdir(\"/homes/bussotti/feverous_work/feverousdata/feverous\")\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "\n",
    "\n",
    "db=FeverousDB(db_path)\n",
    "\n",
    "#Path to the dataset to use\n",
    "f=open(base_path+'/shareddata/feverous_train_challenges_only_tables_evidence_add_filtered_st_v2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the original claims\n",
    "list_claims=[]\n",
    "for line in f:\n",
    "    list_claims+=[json.loads(line)]\n",
    "    \n",
    "list_claims_txt=[]\n",
    "for elt in list_claims:\n",
    "    list_claims_txt+=[elt['claim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_value(txt):\n",
    "    try:\n",
    "        txt=txt.replace('header_cell','cell')\n",
    "        page_to_request=str(txt.split('_')[0])\n",
    "        table_to_request=int(txt.split('_')[2])\n",
    "        row_to_request=int(txt.split('_')[3])\n",
    "        col_to_request=int(txt.split('_')[4])\n",
    "        page_json = db.get_doc_json(page_to_request)\n",
    "        wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "        wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "        wiki_table_0 = wiki_tables[table_to_request]\n",
    "        wiki_table_0_rows = wiki_table_0.get_rows() #return list of WikiRows\n",
    "\n",
    "        cells_row_0 = wiki_table_0_rows[row_to_request].get_row_cells()#return list with WikiCells for row 0\n",
    "        return str(cells_row_0[col_to_request])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.print_exc())\n",
    "        return 'ERROR'\n",
    "    \n",
    "def get_table(page_name, table_nb):\n",
    "\n",
    "    page_json = db.get_doc_json(page_name)\n",
    "    wiki_page = WikiPage(page_name, page_json)\n",
    "\n",
    "    wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "    return [[str(y) for y in x.get_row_cells()] for x in wiki_tables[table_nb].get_rows()]\n",
    "  \n",
    "\n",
    "def get_nb_cols(table):\n",
    "    return min([len(u) for u in table])\n",
    "\n",
    "def get_nb_rows(table):\n",
    "    return len(table)\n",
    "\n",
    "def get_pos(txt):\n",
    "    return [int(x) for x in txt.split('_')[-2:]]\n",
    "\n",
    "def get_comp(elt,elt2):\n",
    "    #equality, inf, sup,...\n",
    "    elt_v=get_cell_value(elt)\n",
    "    elt2_v=get_cell_value(elt2)\n",
    "    \n",
    "    comp=''\n",
    "    if elt_v.replace('.','',1).isdigit() and elt2_v.replace('.','',1).isdigit() :\n",
    "        if float(elt_v)<float(elt2_v):\n",
    "            comp='<'\n",
    "        elif float(elt_v)-float(elt2_v)<1e-5:\n",
    "            comp='='\n",
    "        else:\n",
    "            comp='>'\n",
    "    else:\n",
    "        if elt_v==elt2_v:\n",
    "            comp='id'\n",
    "        else:\n",
    "            comp='diff'\n",
    "    return [elt_v,comp,elt2_v]\n",
    "\n",
    "def get_map_evidence(evidence):\n",
    "    pos_first=None\n",
    "    map_r=[]\n",
    "    relation=dict()\n",
    "    rel=evidence[0]['content']\n",
    "    for elt in rel:\n",
    "        if '_caption_' in elt:\n",
    "            continue\n",
    "        if pos_first==None:\n",
    "            pos_first=get_pos(elt)\n",
    "            map_r+=[[0,0]]\n",
    "        else:\n",
    "            map_r+=[[get_pos(elt)[0]-pos_first[0],get_pos(elt)[1]-pos_first[1]]]\n",
    "    all_diff=True\n",
    "    page_name=''\n",
    "    table_nb=''\n",
    "    for i in range(len(rel)):\n",
    "        elt=rel[i]\n",
    "        if '_caption_' in elt:\n",
    "            continue\n",
    "        res=dict()\n",
    "        for i2 in range(len(rel)):\n",
    "            if i2==i:\n",
    "                continue\n",
    "            elt2=rel[i2]\n",
    "            if '_caption_' in elt2:\n",
    "                continue\n",
    "            res[i2]=get_comp(elt,elt2)\n",
    "            txt=elt.replace('header_cell','cell')\n",
    "            page_name=str(txt.split('_')[0])\n",
    "            table_nb=int(txt.split('_')[2])\n",
    "            if not res[i2][1]=='diff':\n",
    "                all_diff=False\n",
    "        relation[i]=res\n",
    "        same_row=dict()\n",
    "        same_col=dict()\n",
    "    for i in range(len(map_r)):\n",
    "        elt_map=map_r[i]\n",
    "        if map_r[i][0] in same_row:\n",
    "            same_row[map_r[i][0]]+=[i]\n",
    "        else:\n",
    "            same_row[map_r[i][0]]=[i]\n",
    "        if map_r[i][1] in same_col:\n",
    "            same_col[map_r[i][1]]+=[i]\n",
    "        else:\n",
    "            same_col[map_r[i][1]]=[i]\n",
    "    same_col=[x for x in list(same_col.values()) if len(x)>1]\n",
    "    same_row=[x for x in list(same_row.values()) if len(x)>1]\n",
    "    \n",
    "    return {'pos_first':pos_first, 'map_r':map_r, 'relation':relation,'all_diff':all_diff, 'same_row':same_row, 'same_col':same_col,'page_name':page_name, 'table_nb':table_nb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "def pysqldf_p(query,ns):\n",
    "    ns.df=pysqldf(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_comp2(table,elt_i,elt2_i):\n",
    "    #equality, inf, sup,...\n",
    "    elt_v=table[elt_i[0]][elt_i[1]]\n",
    "    #print(elt2_i)\n",
    "    elt2_v=table[elt2_i[0]][elt2_i[1]]\n",
    "    \n",
    "    comp=''\n",
    "    if elt_v.replace('.','',1).isdigit() and elt2_v.replace('.','',1).isdigit() :\n",
    "        if float(elt_v)<float(elt2_v):\n",
    "            comp='<'\n",
    "        elif float(elt_v)-float(elt2_v)<1e-5:\n",
    "            comp='='\n",
    "        else:\n",
    "            comp='>'\n",
    "    else:\n",
    "        if elt_v==elt2_v:\n",
    "            comp='eq'\n",
    "        else:\n",
    "            comp='diff'\n",
    "    return comp\n",
    "\n",
    "def get_graph(evidences,table):\n",
    "    graph=dict()\n",
    "    for i in range(len(evidences)):\n",
    "        graph[i]=dict()\n",
    "        graph[i]['visited']=False\n",
    "        graph[i]['position']=evidences[i]\n",
    "        graph[i]['relation']=[]\n",
    "        graph[i]['same_row']=[]\n",
    "        graph[i]['same_col']=[]\n",
    "        for t in range(len(evidences)):\n",
    "            if t==i:\n",
    "                continue\n",
    "            graph[i]['relation']+=[[t,get_comp2(table,evidences[i],evidences[t])]]\n",
    "            if evidences[i][0]==evidences[t][0]:\n",
    "                graph[i]['same_row']+=[t]\n",
    "            if evidences[i][1]==evidences[t][1]:\n",
    "                graph[i]['same_col']+=[t]\n",
    "    return graph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb07d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(table):\n",
    "    table=[[elt[0]]+elt[1][:] for elt in enumerate(table)]\n",
    "    df=pd.DataFrame(table)\n",
    "    df.columns=['key']+['a'+str(i) for i in range(0,len(table[0])-1)]\n",
    "    return df\n",
    "           \n",
    "def create_df_nok(table):\n",
    "    table=[elt[1][:] for elt in enumerate(table)]\n",
    "    df=pd.DataFrame(table)\n",
    "    df.columns=['key']+['a'+str(i) for i in range(0,len(table[0])-1)]\n",
    "    return df\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_next_not_visited(graph):\n",
    "    for elt in graph.keys():\n",
    "        if graph[elt]['visited']==False:\n",
    "            return elt\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657939fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(graph, df_name):\n",
    "    nb_nodes=len(graph.keys())\n",
    "    pairs_node_srow=[]\n",
    "    for elt in graph.keys():\n",
    "        graph[elt]['visited']=False\n",
    "        res=set([elt]+graph[elt]['same_row'])\n",
    "        if not res in pairs_node_srow:\n",
    "            pairs_node_srow+=[res]\n",
    "\n",
    "    pairs_node_srow_en=list(enumerate(pairs_node_srow))\n",
    "    txt_select='SELECT '\n",
    "    pairs_visited=[]\n",
    "    \n",
    "    txt_where=' WHERE '\n",
    "    txt_from=' FROM '\n",
    "    #FROM\n",
    "    for elt in pairs_node_srow_en:\n",
    "        txt_from+=df_name + ' as t'+str(elt[0])+ ', '\n",
    "    txt_from=txt_from[:-2]\n",
    "    #WHERE\n",
    "    next_not_visited=pick_next_not_visited(graph)\n",
    "    while not next_not_visited==None:\n",
    "        elt=graph[next_not_visited]\n",
    "        graph[next_not_visited]['visited']=True\n",
    "        conv_nnv_to_var=[x[0] for x in pairs_node_srow_en if next_not_visited in x[1]][0]\n",
    "        this_cell_txt='t'+str(conv_nnv_to_var)+ '.a'+str(elt['position'][1])\n",
    "        this_cell_txt_k='t'+str(conv_nnv_to_var)+ '.key'\n",
    "        this_cell_txt_k_old='t'+str(next_not_visited)+ '.key'\n",
    "        txt_select+=this_cell_txt+', '+this_cell_txt_k +', '\n",
    "        for u in elt['relation']:\n",
    "            conv_u_to_var=[x[0] for x in pairs_node_srow_en if u[0] in x[1]][0]\n",
    "            other_cell_txt='t'+str(conv_u_to_var)+ '.a'+str(graph[u[0]]['position'][1])\n",
    "            other_cell_txt_k='t'+str(conv_u_to_var)+ '.key'\n",
    "            other_cell_txt_k_old='t'+str(u[0])+ '.key'\n",
    "            if([this_cell_txt_k_old,other_cell_txt_k_old] in pairs_visited):\n",
    "                continue\n",
    "            pairs_visited+=[[this_cell_txt_k_old,other_cell_txt_k_old],[other_cell_txt_k_old,this_cell_txt_k_old]]\n",
    "            \n",
    "            rel_txt=''\n",
    "            if u[1]=='eq' or u[1]=='=':\n",
    "                rel_txt='='\n",
    "            if u[1]=='<' :\n",
    "                rel_txt='<'\n",
    "            if u[1]=='>' :\n",
    "                rel_txt='>'\n",
    "            if u[1]=='diff' :\n",
    "                rel_txt='<>'    \n",
    "            if not rel_txt=='':\n",
    "                txt_where+= this_cell_txt+rel_txt+other_cell_txt+' and '\n",
    "        if True:\n",
    "            next_not_visited=pick_next_not_visited(graph)\n",
    "    txt_where=txt_where[:-5]\n",
    "    txt_select=txt_select[:-2]\n",
    "    return txt_select +txt_from + txt_where + ' LIMIT 800'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81695a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_array(claim):\n",
    "    evidences=[]\n",
    "    for elt in claim['evidence']:\n",
    "        for elt2 in elt['content']:\n",
    "            if 'table_caption' in elt2:\n",
    "                continue\n",
    "            ev=[int(x) for x in elt2.split('_')[-2:]]\n",
    "            if not ev in evidences:\n",
    "                evidences+=[ev]\n",
    "    return evidences\n",
    "\n",
    "def get_evidence_df(evidence,table):\n",
    "    elts=[]\n",
    "    for u in evidence:\n",
    "        elts+=[table[u[0]][u[1]]]\n",
    "    return create_df([elts])\n",
    "\n",
    "def get_page_name_tb_id(claim):\n",
    "    table_nb=-1\n",
    "    page_name=''\n",
    "    for elt in claim['evidence']:\n",
    "        for elt2 in elt['content']:\n",
    "            if 'table_caption' in elt2:\n",
    "                continue\n",
    "            if not (page_name=='' and table_nb==-1) and not( page_name==elt2.split('_')[0] and table_nb==int(elt2.split('_')[-3])):\n",
    "                return 'Error','Error'\n",
    "            page_name=elt2.split('_')[0]\n",
    "            table_nb=int(elt2.split('_')[-3])\n",
    "    return page_name,table_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0583ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_nit(claim):\n",
    "    ctxt=[]\n",
    "    title=''\n",
    "    section=''\n",
    "    section_nb=-1\n",
    "    \n",
    "    for elt in claim['evidence']:\n",
    "        for elt2 in elt['context']:\n",
    "\n",
    "            for elt3 in elt['context'][elt2]:\n",
    "                if 'cell_' in elt3 :\n",
    "                    continue\n",
    "                    \n",
    "                page_to_request=str(elt3.split('_')[0])\n",
    "                page_json = db.get_doc_json(page_to_request)\n",
    "                wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "                ev='_'.join(elt3.split('_')[1:])\n",
    "\n",
    "                res_txt=wiki_page.get_element_by_id(elt3)\n",
    "                \n",
    "                wiki_tables = wiki_page.get_tables()\n",
    "\n",
    "                if not ev=='title':\n",
    "                    res_txt=str(wiki_page.page_items[ev])\n",
    "                    if 'section' in ev:\n",
    "                        section=str(wiki_page.page_items[ev])\n",
    "                        section_nb=ev.split('_')[1]\n",
    "                else:\n",
    "                    title=page_to_request\n",
    "                    res_txt=page_to_request\n",
    "\n",
    "                ctxt+=[res_txt]\n",
    "             \n",
    "    return list(set(ctxt)),title,section,section_nb\n",
    "    \n",
    "def correct_index_df(df):\n",
    "    df.columns=['a'+str(i) for i in range(0,len(df.columns))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e17cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_no_ctxt(table):\n",
    "    table2=[]\n",
    "    for row in table:\n",
    "        if not len([x for x in row if '[H]' not in x])==0:\n",
    "            table2+=[row]\n",
    "    return table2\n",
    "\n",
    "\n",
    "\n",
    "def table_no_ctxt_k(table):\n",
    "    table2=[]\n",
    "    for row_e in enumerate(table):\n",
    "        row=row_e[1]\n",
    "        row=[row_e[0]]+row\n",
    "        if not len([x for x in row[1:] if '[H]' not in x])==0:\n",
    "            table2+=[row]\n",
    "    return table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cells_value(list_cells):\n",
    "    return [get_cell_value(x) for x in list_cells]\n",
    "\n",
    "def get_all_headers_with_id(table):\n",
    "    res=[]\n",
    "    for row in enumerate(table):\n",
    "        for col in enumerate(row[1]):\n",
    "            if '[H]' in col[1]:\n",
    "                cell_v=col[1]\n",
    "                if '[[' in cell_v and '|' in cell_v:\n",
    "                    cell_v=cell_v.split('|')[1].split(']]')[0]\n",
    "\n",
    "                res+= [[cell_v.replace('[H] ',''),[row[0],col[0]]]]\n",
    "    return res\n",
    "\n",
    "def get_headers_used_in_ev(claim):\n",
    "    res=[]\n",
    "    for ev in claim['evidence'][0]['content']:\n",
    "        for ctxt_cell in claim['evidence'][0]['context'][ev]:\n",
    "            if '_title' in ctxt_cell or '_section' in ctxt_cell  or '_caption' in ctxt_cell:\n",
    "                continue\n",
    "            cell_v=get_cell_value(ctxt_cell)\n",
    "            if '[[' in cell_v and '|' in cell_v:\n",
    "                cell_v=cell_v.split('|')[1].split(']]')[0]\n",
    "            to_add=[cell_v.replace('[H] ',''),get_pos(ctxt_cell)]\n",
    "            if not to_add in res:\n",
    "                res+=[to_add]\n",
    "    return sorted(res,key=lambda x: str(x[1][0])+'_'+str(x[1][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481039df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def shuffle_col(col_id,df2):\n",
    "    df_mixed=copy.deepcopy(df2)\n",
    "    col_to_modify=list(df2.loc[:,col_id])\n",
    "    values=set(col_to_modify)\n",
    "    rows_to_drop=[]\n",
    "    if(len(list(values))<2):\n",
    "        return -1\n",
    "    for i in range(len(col_to_modify)):\n",
    "        values_for=copy.deepcopy(list(values))\n",
    "        values_for=[x for x in values_for if not x==df2.loc[i,col_id]]\n",
    "        while((df_mixed.loc[i,:]==df2).all(1).any() and not len(values_for)==0):\n",
    "            chosen=random.choice(values_for)\n",
    "            df_mixed.loc[i,col_id]=chosen\n",
    "            values_for=[x for x in values_for if not x==chosen]\n",
    "        if (df_mixed.loc[i,:]==df2).all(1).any():\n",
    "            rows_to_drop+=[i]\n",
    "    if len(rows_to_drop)>0:\n",
    "        df_mixed=df_mixed.drop(rows_to_drop)\n",
    "    return df_mixed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def identify_label_cat(claim):\n",
    "    list_of_words=['min ','minimum', 'lowest', 'smallest', 'slowest', 'shortest']\n",
    "    words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "    if words_re.search(claim):\n",
    "        return 'min'\n",
    "        \n",
    "    list_of_words=['max ', 'maximum', 'highest', 'biggest', 'fastest', 'longest', 'tallest']\n",
    "    words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "    if words_re.search(claim):\n",
    "        return 'max'\n",
    "        \n",
    "        \n",
    "    list_of_words=['average', 'mean']\n",
    "    words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "    if words_re.search(claim):\n",
    "        return 'average'\n",
    "        \n",
    "    list_of_words=['greater .*than', 'bigger .*than','taller .*than','smaller .*than','better .*than','shorter .*than','lower .*than','higher .*than','faster .*than','slower .*than','less .*than','more .*than']\n",
    "    words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "    if words_re.search(claim):\n",
    "        return 'compare'\n",
    "        \n",
    "        \n",
    "    list_of_words=['count', 'three','four','five','six','seven','eight','nine']\n",
    "    words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "    if words_re.search(claim):\n",
    "        return 'count'\n",
    "        \n",
    "    return 'lookup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico={'min':0,'max':0,'average':0,'compare':0,'count':0,'lookup':0}\n",
    "for elt in list_claims[1:]:\n",
    "    claim=identify_label_cat(elt['claim'])\n",
    "    dico[claim]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6257b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_seed=40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929ca3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finishedthis=False\n",
    "txt=''\n",
    "res=[]\n",
    "counts={'processed':0,'timetoolong':0,'alldiff':0,'alldiffcolrow':0,'multipages':0,'toomuchctxt':0,'noctxt':0,'qtoolong':0,}\n",
    "counts_alternative={'processed_query_identical':0,'timetoolong':0,'processed_query_diff':0,'key_pb':0,'contextgather_pb':0,}\n",
    "totaltime=time()\n",
    "for i in range(0,len(list_claims)):\n",
    "    print(i)\n",
    "    try:\n",
    "        if not list_claims[i]['label']=='SUPPORTS':\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        previous_met=get_map_evidence(list_claims[i]['evidence'])\n",
    "        \n",
    "        if(len(previous_met['same_row'])==0 and previous_met['all_diff'] and len(previous_met['same_col'])==0 ): #\n",
    "            counts['alldiffcolrow']+=1\n",
    "            print('The claim '+str(i)+' has no cells being on same row or column, and no common values, skipping')# \n",
    "            continue\n",
    "        claim=list_claims[i]\n",
    "        evidence=get_evidence_array(claim)\n",
    "        ctxt_nit,title,section,section_nb=get_context_nit(claim)\n",
    "        page_name,table_nb=get_page_name_tb_id(claim)\n",
    "        if [page_name,table_nb]==['Error','Error']:\n",
    "            print('The claim '+str(i)+' may be using two pages or tables, skipping')\n",
    "            counts['multipages']+=1\n",
    "            continue\n",
    "        table=get_table(page_name, table_nb)\n",
    "        graph=get_graph(evidence,table)\n",
    "        query=create_query_shortershorter(graph, 'df')\n",
    "        df_wctxt=create_df(table)\n",
    "        table2=table_no_ctxt_k(table)\n",
    "        df=create_df_nok(table2)\n",
    "        if(len(query)>4000):\n",
    "            print('Query for claim '+str(i)+ ' too long, skipping')\n",
    "            counts['qtoolong']+=1\n",
    "            continue\n",
    "        mgr = multiprocessing.Manager()\n",
    "        ns = mgr.Namespace()\n",
    "        ns.df =pd.DataFrame()\n",
    "\n",
    "        p = multiprocessing.Process(target=pysqldf_p, name=\"Exe\", args=(query,ns))\n",
    "        p.start()\n",
    "        p.join(20)\n",
    "        \n",
    "        \n",
    "        if p.is_alive():\n",
    "            print(\"Query took too long, killing\")\n",
    "\n",
    "            # Terminate \n",
    "            p.terminate()\n",
    "            p.join()\n",
    "            continue\n",
    "        else:\n",
    "            df2=ns.df\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        print('Query ran')\n",
    "        df2=correct_index_df(df2)\n",
    "       \n",
    "        evidence_df=get_evidence_df(evidence,table).drop(['key'],axis=1)\n",
    "        cols_used=[graph[x]['position'][1] for x in graph.keys()]\n",
    "\n",
    "        \n",
    "        df_new=copy.deepcopy(df2)\n",
    "        values_col=list(df2.columns)[::2]\n",
    "        nb_shuffled=0\n",
    "        nb_to_shuffle=int(len(values_col)/2)\n",
    "        colshuffled=None\n",
    "        while(not len(values_col)==0 and nb_shuffled<nb_to_shuffle):\n",
    "            choosencol=random.choice(values_col)\n",
    "\n",
    "            df_new_2=shuffle_col(choosencol,df_new)\n",
    "            if not type(df_new_2)==int:\n",
    "                df_new=df_new_2\n",
    "                nb_shuffled+=1\n",
    "\n",
    "\n",
    "            values_col=[x for x in values_col if not x==choosencol]\n",
    "        \n",
    "        if(not type(df_new)==int):\n",
    "            res_shuffled_original=df_new\n",
    "            colshuffled=choosencol\n",
    "        else:\n",
    "            res_shuffled_original=None\n",
    "\n",
    "        alt_tables=False\n",
    "        alternative_tables=[]\n",
    "       \n",
    "\n",
    "        claim_type=identify_label_cat(claim['claim'])\n",
    "        \n",
    "        res+=[{'original_claim':claim,'claim_type':claim_type,'colshuffled':colshuffled,'res_shuffled':res_shuffled_original,'alt_tables':alt_tables,'alternative_tables':alternative_tables,'original_label':list_claims[i]['label'],'alldiff':previous_met['all_diff'],'cols_used':cols_used,'query':query,'res':df2, 'nb_variables':max(graph.keys()),'title':title, 'section':section,'section_nb':section_nb,'table_nb':table_nb, 'ctxt_nit':ctxt_nit,'evidence_df':evidence_df,'table':df, 'df_wctxt':df_wctxt,'previous_met':previous_met,'i':i}]\n",
    "        print('Query for claim '+str(i)+ ' processed')\n",
    "        counts['processed']+=1\n",
    "        \n",
    "        \n",
    "        if len(res)>nb_seed:\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error')\n",
    "        print(e)\n",
    "\n",
    "\n",
    "finishedthis=True\n",
    "totaltime=time()-totaltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finishedthis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42973604",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltime/counts['processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_value_noreplace(title, table, col):\n",
    "    try:\n",
    "        \n",
    "        page_to_request=title\n",
    "        table_to_request=table\n",
    "        col_to_request=col\n",
    "        page_json = db.get_doc_json(page_to_request)\n",
    "        wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "        wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "        wiki_table_0 = wiki_tables[table_to_request]\n",
    "        wiki_table_0_rows = wiki_table_0.get_rows() #return list of WikiRows\n",
    "\n",
    "        cells_col=[str(wiki_table_0_rows[x].get_row_cells()[col]) for x in range(len(wiki_table_0_rows))]\n",
    "        \n",
    "        return cells_col\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.print_exc())\n",
    "        return 'ERROR'\n",
    "    \n",
    "    \n",
    "def get_row_value_noreplace(title, table, row):\n",
    "    try:\n",
    "        \n",
    "        page_to_request=title\n",
    "        table_to_request=table\n",
    "        page_json = db.get_doc_json(page_to_request)\n",
    "        wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "        wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "        wiki_table_0 = wiki_tables[table_to_request]\n",
    "        wiki_table_0_rows = wiki_table_0.get_rows() #return list of WikiRows\n",
    "\n",
    "        cells_row=[str(wiki_table_0_rows[row].get_row_cells()[col]) for col in range(len(list(wiki_table_0_rows[row].get_row_cells())))]\n",
    "        \n",
    "        return cells_row\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.print_exc())\n",
    "        return 'ERROR'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_value_noreplace_fromtablevalues(table_values, col):\n",
    "    cells_col=[x[col] for x in table_values]\n",
    "    return cells_col\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_row_value_noreplace_fromtablevalues(table_values, row):\n",
    "    return table_values[row]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e37fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_value_noreplace(txt):\n",
    "    try:\n",
    "        \n",
    "        page_to_request=str(txt.split('_')[0])\n",
    "        table_to_request=int(txt.split('_')[-3])\n",
    "        row_to_request=int(txt.split('_')[-2])\n",
    "        col_to_request=int(txt.split('_')[-1])\n",
    "        page_json = db.get_doc_json(page_to_request)\n",
    "        wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "        wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "        wiki_table_0 = wiki_tables[table_to_request]\n",
    "        wiki_table_0_rows = wiki_table_0.get_rows() #return list of WikiRows\n",
    "\n",
    "        cells_row_0 = wiki_table_0_rows[row_to_request].get_row_cells()#return list with WikiCells for row 0\n",
    "        return str(cells_row_0[col_to_request])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.print_exc())\n",
    "        return 'ERROR'\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correct_ev_dict_v2(evidence_dict):\n",
    "    page_to_request=list(evidence_dict.keys())[0].split('_')[0]\n",
    "    table_to_request=int(list(evidence_dict.keys())[0].split('_')[2])\n",
    "\n",
    "    page_json = db.get_doc_json(page_to_request)\n",
    "    wiki_page = WikiPage(page_to_request, page_json)\n",
    "\n",
    "    wiki_tables = wiki_page.get_tables() #return list of all Wiki Tables\n",
    "\n",
    "    wiki_table_0 = wiki_tables[table_to_request]\n",
    "    list_cells=list(wiki_table_0.all_cells.keys())\n",
    "    new_dict=dict()\n",
    "    for elt in evidence_dict.keys():\n",
    "        values=[]\n",
    "        for value in evidence_dict[elt]:\n",
    "            if '_title' in value or 'section_' in value:\n",
    "                values+=[value]\n",
    "                continue\n",
    "            cellv='_'.join(value.split('_')[1:])\n",
    "            headerv='header_'+cellv\n",
    "            if cellv in list_cells:\n",
    "                values+=[value]\n",
    "            elif headerv in list_cells:\n",
    "                values+=[page_to_request+'_'+headerv]\n",
    "            else:\n",
    "\n",
    "                tmpval=get_cell_value_noreplace(elt)\n",
    "                tmpval=tmpval.replace('[H] ','')\n",
    "                notfound=True\n",
    "                if not tmpval=='ERROR':\n",
    "                    dico=wiki_table_0.all_cells\n",
    "                    \n",
    "                    for candicell in dico.keys():\n",
    "                        if dico[candicell].content==tmpval:\n",
    "                            \n",
    "                            values+=[page_to_request+'_'+candicell]\n",
    "                            notfound=False\n",
    "                            break\n",
    "                if tmpval=='ERROR' or notfound:\n",
    "                    print('ERROR '+value)\n",
    "                    print('1!!!!!!!!!!!!not corrected//skip, name: '+value+', value: '+tmpval)\n",
    "                    continue\n",
    "                \n",
    "        cellv='_'.join(elt.split('_')[1:])\n",
    "        headerv='header_'+cellv\n",
    "        if cellv in list_cells:\n",
    "            name=elt\n",
    "        elif headerv in list_cells:\n",
    "            name=page_to_request+'_'+headerv\n",
    "        else:\n",
    "            name=elt\n",
    "            tmpval=get_cell_value_noreplace(elt)\n",
    "            tmpval=tmpval.replace('[H] ','')\n",
    "            notfound=True\n",
    "            if not tmpval=='ERROR':\n",
    "                dico=wiki_table_0.all_cells\n",
    "\n",
    "                for candicell in dico.keys():\n",
    "                    if dico[candicell].content==tmpval:\n",
    "                        \n",
    "                        name=page_to_request+'_'+candicell\n",
    "                        notfound=False\n",
    "                        break\n",
    "            if tmpval=='ERROR' or notfound:\n",
    "                print('ERROR '+elt)\n",
    "                print('2!!!!!!!!!!!!not corrected//skip, name: '+name+', value: '+tmpval)\n",
    "                continue\n",
    "           \n",
    "           \n",
    "        new_dict[name]=values\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa667ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_contains_letters(w):\n",
    "    return w.upper().isupper()#.isupper() or w.islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fe80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "def remove_add_row(table):\n",
    "    \n",
    "    if random.choice([1,2])==1 and not len(table)==1:\n",
    "        #remove\n",
    "        to_rm=random.choice(range(len(table)))\n",
    "        return [x[1] for x in enumerate(table) if not x[0]==to_rm]\n",
    "    else:\n",
    "        to_add=copy.deepcopy(table[random.choice(range(len(table)))])\n",
    "        for i in range(len(to_add)):\n",
    "            col_i_is_int=True\n",
    "            \n",
    "            \n",
    "            col_i_ctxt=[]\n",
    "            #############In case we want to add the context when generating a fake empty cell\n",
    "            \n",
    "            if len(to_add[i][0])==0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            gen_words=[]\n",
    "            for [to_test_val,tval_ctxt] in [x[i] for x in table if len(x[i][0])>0]:\n",
    "                if len(gen_words)>4:\n",
    "                    continue\n",
    "                ####TO NOT GEN TOO MUCH USELESS FLANT5 ALTERNATIVES\n",
    "                if(len(tval_ctxt))>0:\n",
    "                    col_i_ctxt+=[tval_ctxt]\n",
    "                or_word=to_test_val.capitalize()\n",
    "                if not or_word.replace('+','').replace('-','').replace('.','').replace(',','').isdigit():\n",
    "                    col_i_is_int=False\n",
    "                input_text = \"Answer the following question by giving me antonyms. Can you give me an antonym of \"+or_word+\"?\"\n",
    "                input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "                outputs = model.generate(input_ids)\n",
    "                gen_word=tokenizer.decode(outputs[0]).replace('<pad> ','').replace('</s>','')\n",
    "                if len(gen_word)>0 and not or_word.lower() in gen_word.lower(): \n",
    "                    gen_words+=[[or_word,gen_word,len(set(or_word)-set(gen_word))+len(set(gen_word)-set(or_word))]]\n",
    "            if len(gen_words)==0:\n",
    "                for gen_word in ['potato','1789','Andrew']:\n",
    "                    gen_words+=[[or_word,gen_word,len(set(or_word)-set(gen_word))+len(set(gen_word)-set(or_word))]]\n",
    "            gen_words=sorted(gen_words,key=lambda x:x[-1])\n",
    "            selected_replacement=gen_words[-1]\n",
    "\n",
    "            res=selected_replacement[1]\n",
    "            if col_i_is_int and not(res.replace('+','').replace('-','').replace('.','').replace(',','').isdigit()):\n",
    "                res=random.randint(1,5000)\n",
    "          \n",
    "            to_add[i][0]=res\n",
    "            #print(\"Same lexical field: \"+allvals+\" and\"+to_add[i][0])\n",
    "        pos_to_add=random.choice(range(len(table)))\n",
    "        table=table[:pos_to_add]+[to_add]+table[pos_to_add:]\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b663e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_to_keep=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_alt_tk=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49be913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tables_for_t5_withnegimp(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb,shuffled_col):\n",
    "    #Works with key added and new format\n",
    "    tables=[]\n",
    "    original_one=[]\n",
    "    table_list=df.values.tolist()\n",
    "    table_shuffled_list=df_shuffled.values.tolist()\n",
    "    evidence=evidence_df.values.tolist()[0]\n",
    "    table_values=get_table(title,table_nb)\n",
    "    textdebug=\"\"\n",
    "    nb_gen_claim=0\n",
    "    previous_evidence=[]\n",
    "    \n",
    "    shuffledcol_id=[elt[0] for elt in enumerate(list(df.columns)) if elt[1]==shuffled_col][0]/2\n",
    "    \n",
    "    \n",
    "    for row_g in zip(table_list,table_shuffled_list):\n",
    "        row=row_g[0]\n",
    "        row_neg=row_g[1]\n",
    "        col_nb_used=[]\n",
    "        row_nb_used=[]\n",
    "        table_dict=dict()\n",
    "        table_neg_dict=dict()\n",
    "        evidence_dict={}\n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row]:\n",
    "            continue\n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row_neg]:\n",
    "            continue\n",
    "        row_val=row[::2]\n",
    "        row_id=row[1::2]\n",
    "        \n",
    "        row_neg_val=row_neg[::2]\n",
    "        row_neg_id=row_neg[1::2]\n",
    "        \n",
    "        \n",
    "\n",
    "        for elt in enumerate(zip(row_val,row_id)):\n",
    "            col_nb=cols_used[elt[0]]#context_id_list[elt[0]][1]\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:elt[1][1]]) if '[H]' in x[1] ]\n",
    "            row_nb=elt[1][1]\n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:row_nb]) if '[H]' in x[1] ]\n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            value=elt[1][0]\n",
    "            row_nb_used+=[row_nb]\n",
    "            col_nb_used+=[col_nb]\n",
    "            if row_nb in table_dict:\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_dict[row_nb]=dict()\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            \n",
    "            evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]=[title+'_title']\n",
    "            \n",
    "            if not section_nb==-1:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_section_'+str(section_nb)]\n",
    "            for ctxt_row_elt in ctxt_in_row:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(ctxt_row_elt[0])]\n",
    "            for ctxt_col_elt in ctxt_in_col:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(ctxt_col_elt[0])+'_'+str(col_nb)]\n",
    "\n",
    "        for elt in enumerate(zip(row_neg_val,row_neg_id)):\n",
    "            col_nb=cols_used[elt[0]]\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:elt[1][1]]) if '[H]' in x[1] ]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            row_nb=elt[1][1]\n",
    "            \n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:row_nb]) if '[H]' in x[1] ]\n",
    "            \n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            value=elt[1][0]\n",
    "\n",
    "            if row_nb in table_neg_dict:\n",
    "                table_neg_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_neg_dict[row_nb]=dict()\n",
    "                table_neg_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "\n",
    "        ######Build original table\n",
    "        table_g=[]\n",
    "        table_neg_g=[]\n",
    "        col_nb_used=list(set(col_nb_used))\n",
    "        row_nb_used=list(set(row_nb_used))\n",
    "        for row in sorted(row_nb_used):\n",
    "            row_g=[]\n",
    "            row_neg_g=[]\n",
    "            for col in sorted(col_nb_used):\n",
    "                if col in table_dict[row]:\n",
    "                    val=table_dict[row][col]\n",
    "                    val_n=table_neg_dict[row][col]\n",
    "                else:\n",
    "                    val=['','']\n",
    "                    val_n=['','']\n",
    "                row_g+=[val]\n",
    "                row_neg_g+=[val_n]\n",
    "            table_g+=[row_g]\n",
    "            table_neg_g+=[row_neg_g]\n",
    "\n",
    "        evidence_dict=correct_ev_dict_v2(evidence_dict)\n",
    "\n",
    "        evi_ctnt=list(evidence_dict.keys())\n",
    "        dropped=False\n",
    "        for elt_prev_evi in previous_evidence:\n",
    "\n",
    "            if(len([x for x in evi_ctnt if x not in elt_prev_evi])==0):\n",
    "                dropped=True\n",
    "                break\n",
    "        if dropped:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        textdebug+=\"\\n\\n\\n%%%%%%%%%%%%%row_val%%%%%%%%%%%%%\\n\"\n",
    "        textdebug+=str(row_val)\n",
    "        textdebug+=\"\\n%%%%%%%%%%%%%evidence%%%%%%%%%%%%%\\n\"\n",
    "        textdebug+=str(evidence)\n",
    "        \n",
    "        if row_val == evidence:\n",
    "\n",
    "            previous_evidence+=[evi_ctnt]\n",
    "            table_neg_g=remove_add_row(table_neg_g)\n",
    "            original_one={'table':table_g,'neg_table':table_neg_g,'evidence_dict':evidence_dict}#,'evidence_dict_neg':evidence_dict_neg}\n",
    "\n",
    "        else:\n",
    "            if not nb_gen_claim>nb_to_keep:\n",
    "                previous_evidence+=[evi_ctnt]\n",
    "                table_neg_g=remove_add_row(table_neg_g)\n",
    "                tables+=[{'table':table_g,'neg_table':table_neg_g,'evidence_dict':evidence_dict}]#,'evidence_dict_neg':evidence_dict_neg}] \n",
    "                nb_gen_claim+=1\n",
    "\n",
    "    return {'generated':tables,'original_one':original_one}#, 'dd':context_id_list}\n",
    "\n",
    "\n",
    "def gen_tables_for_t5_withnegimp_mp(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb,ns,shuffledcol):\n",
    "    ns.ret=gen_tables_for_t5_withnegimp(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb,shuffledcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_tables_for_t5_alternative_impneg(df, df_shuffled,cols_used,title,table_nb,shuffled_col):\n",
    "    #Works with key added and new format\n",
    "    tables=[]\n",
    "    table_list=df.values.tolist()\n",
    "    table_shuffled_list=df_shuffled.values.tolist()\n",
    "    table_values=get_table(title,table_nb)\n",
    "    nb_gen_claim=0\n",
    "    previous_evidence=[]\n",
    "    \n",
    "    shuffledcol_id=[elt[0] for elt in enumerate(list(df.columns)) if elt[1]==shuffled_col][0]/2\n",
    "    \n",
    "    for row_g in zip(table_list,table_shuffled_list):\n",
    "        row=row_g[0]\n",
    "        row_neg=row_g[1]\n",
    "        col_nb_used=[]\n",
    "        row_nb_used=[]\n",
    "        table_dict=dict()\n",
    "        evidence_dict={}\n",
    "\n",
    "        table_neg_dict=dict()\n",
    "\n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row]:\n",
    "            continue\n",
    "            \n",
    "    \n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row_neg]:\n",
    "            continue\n",
    "        row_val=row[::2]\n",
    "        row_id=row[1::2]\n",
    "        \n",
    "        row_neg_val=row_neg[::2]\n",
    "        row_neg_id=row_neg[1::2]\n",
    "        \n",
    "        \n",
    "        for elt in enumerate(zip(row_val,row_id)):\n",
    "\n",
    "            col_nb=cols_used[elt[0]]#context_id_list[elt[0]][1]\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:elt[1][1]]) if '[H]' in x[1] ]\n",
    "            row_nb=elt[1][1]\n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:row_nb]) if '[H]' in x[1] ]\n",
    "\n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            \n",
    "            \n",
    "            value=elt[1][0]\n",
    "            row_nb_used+=[row_nb]\n",
    "            col_nb_used+=[col_nb]\n",
    "            if row_nb in table_dict:\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_dict[row_nb]=dict()\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "                \n",
    "            evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]=[title+'_title']\n",
    "            \n",
    "            if not section_nb==-1:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_section_'+str(section_nb)]\n",
    "            for ctxt_row_elt in ctxt_in_row:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(ctxt_row_elt[0])]\n",
    "            for ctxt_col_elt in ctxt_in_col:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(ctxt_col_elt[0])+'_'+str(col_nb)]\n",
    "\n",
    "        for elt in enumerate(zip(row_neg_val,row_neg_id)):\n",
    "            col_nb=cols_used[elt[0]]#context_id_list[elt[0]][1]\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:elt[1][1]]) if '[H]' in x[1] ]\n",
    "            row_nb=elt[1][1]\n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:row_nb]) if '[H]' in x[1] ]\n",
    "\n",
    "            \n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            value=elt[1][0]\n",
    "\n",
    "            if row_nb in table_neg_dict:\n",
    "                table_neg_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_neg_dict[row_nb]=dict()\n",
    "                table_neg_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "\n",
    "        \n",
    "        ######Build original table\n",
    "        table_g=[]\n",
    "        table_neg_g=[]\n",
    "        col_nb_used=list(set(col_nb_used))\n",
    "        row_nb_used=list(set(row_nb_used))\n",
    "\n",
    "        for row in sorted(row_nb_used):\n",
    "            row_g=[]\n",
    "            row_neg_g=[]\n",
    "            for col in sorted(col_nb_used):\n",
    "                if col in table_dict[row]:\n",
    "\n",
    "                    val=table_dict[row][col]\n",
    "                    val_n=table_neg_dict[row][col]\n",
    "                else:\n",
    "                    val=['','']\n",
    "                    val_n=['','']\n",
    "                row_g+=[val]\n",
    "                row_neg_g+=[val_n]\n",
    "            table_g+=[row_g]\n",
    "            table_neg_g+=[row_neg_g]\n",
    "\n",
    "        evidence_dict=correct_ev_dict_v2(evidence_dict)\n",
    "\n",
    "        \n",
    "        evi_ctnt=list(evidence_dict.keys())\n",
    "\n",
    "        dropped=False\n",
    "        for elt_prev_evi in previous_evidence:\n",
    "\n",
    "            if(len([x for x in evi_ctnt if x not in elt_prev_evi])==0):\n",
    "                dropped=True\n",
    "\n",
    "                break\n",
    "\n",
    "        if dropped:\n",
    "            continue\n",
    "        previous_evidence+=[evi_ctnt]\n",
    "        if not nb_gen_claim>nb_to_keep:\n",
    "            table_neg_g=remove_add_row(table_neg_g)\n",
    "            tables+=[{'table':table_g,'neg_table':table_neg_g,'evidence_dict':evidence_dict}]#,'evidence_dict_neg':evidence_dict_neg}] \n",
    "            nb_gen_claim+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "    return tables\n",
    "\n",
    "\n",
    "\n",
    "def gen_tables_for_t5_alternative_impneg_mp(df, df_shuffled,cols_used,title,table_nb,ns,shuffledcol):\n",
    "    ns.ret=gen_tables_for_t5_alternative_impneg(df, df_shuffled,cols_used,title,table_nb,shuffledcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e31a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elts=[]\n",
    "for i in range(1,len(res)):\n",
    "    if len(elts)>nb_seed:\n",
    "        break\n",
    "    print(i)\n",
    "\n",
    "    title=res[i]['title']\n",
    "    section_nb=res[i]['section_nb']\n",
    "    table_nb=res[i]['table_nb']\n",
    "\n",
    "    if not 'res_shuffled' in res[i].keys() or type(res[i]['res_shuffled'])==type(None):\n",
    "        print('No neg, skip')\n",
    "        continue\n",
    "    \n",
    "\n",
    "    new_elt=gen_tables_for_t5_withnegimp(res[i]['res'],res[i]['res_shuffled'], res[i]['cols_used'],res[i]['evidence_df'],title,section_nb,table_nb,res[i]['colshuffled'])\n",
    "    \n",
    "    if(len(res[i]['alternative_tables'])>0):\n",
    "        alt_broken=0\n",
    "        new_elt['tables_alternative']=[]\n",
    "        for j in range(0,len(res[i]['alternative_tables'])):\n",
    "            if alt_broken>3:\n",
    "                print('Alternative gen stopped')\n",
    "                break\n",
    "            if(j%20==0):\n",
    "                print(str(i)+','+str(j))\n",
    "            if nb_alt_tk<j and not nb_alt_tk==-1:\n",
    "                break\n",
    "            title=res[i]['alternative_tables'][j]['title']\n",
    "            table_nb=res[i]['alternative_tables'][j]['table_nb']\n",
    "            if type(res[i]['alternative_tables'][j]['res_shuffled'])==type(None):\n",
    "                print('No negative for ('+str(i)+','+str(j)+'), skipping')\n",
    "                continue\n",
    "                \n",
    "\n",
    "            new_elt_alt=gen_tables_for_t5_alternative_impneg(res[i]['alternative_tables'][j]['res'], res[i]['alternative_tables'][j]['res_shuffled'],res[i]['alternative_tables'][j]['cols_used'],title,table_nb,res[i]['alternative_tables'][j]['colshuffled'])\n",
    "            \n",
    "            new_elt['tables_alternative']+=[{'table':new_elt_alt, 'title':title,'table_nb':table_nb}]\n",
    "    new_elt['original_claim']=res[i]['original_claim']['claim']\n",
    "    new_elt['ctxt_nit']=res[i]['ctxt_nit']\n",
    "    new_elt['title']=res[i]['title']\n",
    "    new_elt['query']=res[i]['query']\n",
    "    new_elt['original_table']=get_table(res[i]['title'],res[i]['table_nb'])\n",
    "    \n",
    "    new_elt['original_label']=res[i]['original_label']\n",
    "    new_elt['alldiff']=res[i]['alldiff']\n",
    "    new_elt['alt_tables']=res[i]['alt_tables']\n",
    "    new_elt['nb_variables']=res[i]['nb_variables']\n",
    "    new_elt['claim_type']=res[i]['claim_type']\n",
    "    if 'i' in list(res[i].keys()):\n",
    "        new_elt['i']=res[i]['i'] \n",
    "    \n",
    "    new_elt['section']=res[i]['section']\n",
    "    \n",
    "    elts+=[new_elt]\n",
    "    \n",
    "    \n",
    "\n",
    "f2=open(base_path+'/evidences_sel_g10_atleastone_240323_query.json','w')\n",
    "\n",
    "json.dump(elts,f2)\n",
    "f2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_tables_for_t5_withneg_random_impneg(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb, shuffledcol):\n",
    "    #Works with key added and new format\n",
    "    tables=[]\n",
    "    original_one=[]\n",
    "    all_ev_dict_keys=[]\n",
    "    cnt_evdone=0\n",
    "    cnt_reppb=0\n",
    "    table_list=df.values.tolist()\n",
    "    table_shuffled_list=df_shuffled.values.tolist()\n",
    "\n",
    "    evidence=evidence_df.values.tolist()[0]\n",
    "    table_values=get_table(title,table_nb)\n",
    "    nb_gen_claim=0\n",
    "    \n",
    "    \n",
    "    totalnbcols=len(table_values[0])\n",
    "    totalnbrows=len(table_values)\n",
    "    \n",
    "    for row_g in zip(table_list,table_shuffled_list):\n",
    "        row=row_g[0]\n",
    "        row_neg=row_g[1]\n",
    "        col_nb_used=[]\n",
    "        row_nb_used=[]\n",
    "        table_dict=dict()\n",
    "        table_neg_dict=dict()\n",
    "        evidence_dict={}\n",
    "\n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row]:\n",
    "            continue\n",
    "        if 0 in [len(str(x).replace('\\u200b','')) for x in row_neg]:\n",
    "            continue\n",
    "        row_val=row[::2]\n",
    "        row_id=row[1::2]\n",
    "        \n",
    "        row_neg_val=row_neg[::2]\n",
    "        row_neg_id=row_neg[1::2]\n",
    "        \n",
    "        allrowspicked=[]\n",
    "        allcolspicked=[]\n",
    "        index_picked=[]\n",
    "        for elt in enumerate(zip(row_val,row_id)):\n",
    "            randomcol=-1\n",
    "            randomrow=-1\n",
    "            value=None\n",
    "            nb_tentative=0\n",
    "            while(value==None and nb_tentative<50):\n",
    "                nb_tentative+=1\n",
    "                randomcol=random.randint(0,totalnbcols-1)\n",
    "                randomrow=random.randint(0,totalnbrows-1)\n",
    "                value=table_values[randomrow][randomcol]\n",
    "                if '[H]' in value:\n",
    "                    value=None\n",
    "                if (randomrow,randomcol) in index_picked:\n",
    "                    value=None\n",
    "            if nb_tentative>=50:\n",
    "                return 'NO3'\n",
    "            index_picked+=[(randomrow,randomcol)]\n",
    "            \n",
    "            col_nb=randomcol#cols_used[elt[0]]#context_id_list[elt[0]][1]\n",
    "            row_nb=randomrow#elt[1][1]\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:row_nb]) if '[H]' in x[1] ]\n",
    "            \n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:col_nb]) if '[H]' in x[1] ]\n",
    "\n",
    "            \n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            \n",
    "            row_nb_used+=[row_nb]\n",
    "            col_nb_used+=[col_nb]\n",
    "            \n",
    "            allrowspicked+=[row_nb]\n",
    "            allcolspicked+=[col_nb]\n",
    "            if row_nb in table_dict:\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_dict[row_nb]=dict()\n",
    "                table_dict[row_nb][col_nb]=[value,ctxt_txt]\n",
    "            \n",
    "            evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]=[title+'_title']\n",
    "            if not section_nb==-1:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_section_'+str(section_nb)]\n",
    "            for ctxt_row_elt in ctxt_in_row:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(ctxt_row_elt[0])]\n",
    "            for ctxt_col_elt in ctxt_in_col:\n",
    "                evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_nb)+'_'+str(col_nb)]+=[title+'_cell_'+str(table_nb)+'_'+str(ctxt_col_elt[0])+'_'+str(col_nb)]\n",
    "\n",
    "        neg_index_picked=[]\n",
    "        nb_to_change=int(len(index_picked)/2)\n",
    "        random_ev_to_mod=[]\n",
    "        while(len(set(random_ev_to_mod))<nb_to_change):\n",
    "            random_ev_to_mod+=[random.randint(0,len(index_picked)-1)]\n",
    "        \n",
    "        random_ev_to_mod=list(set(random_ev_to_mod))\n",
    "        \n",
    "        for elt in enumerate(zip(row_neg_val,row_neg_id)):\n",
    "            randomcol=-1\n",
    "            randomrow=-1\n",
    "            value=None\n",
    "            randomcol=index_picked[elt[0]][1]\n",
    "            randomrow=index_picked[elt[0]][0]\n",
    "            value=table_values[randomrow][randomcol]\n",
    "            values=[]\n",
    "            nb_tentative =0\n",
    "            \n",
    "            if elt[0] in random_ev_to_mod:\n",
    "                original_value=table_values[randomrow][randomcol]\n",
    "                value=None\n",
    "            while(value==None and nb_tentative<30):\n",
    "                randomcol=random.randint(0,totalnbcols-1)\n",
    "                randomrow=random.randint(0,totalnbrows-1)\n",
    "                value=table_values[randomrow][randomcol]\n",
    "                nb_tentative+=1\n",
    "                if '[H]' in value:\n",
    "                    value=None\n",
    "                if (randomrow,randomcol) in index_picked:\n",
    "                    value=None\n",
    "                if original_value==value:\n",
    "                    value=None\n",
    "                    \n",
    "                \n",
    "            if nb_tentative>=30:\n",
    "                print('################problem_here')\n",
    "                cnt_reppb+=1\n",
    "                if cnt_reppb>25:\n",
    "                    return 'NO2'\n",
    "                value=random.choice(['4535','325','54212','1246', '2454','56','24', '776','9754','1', '14'])\n",
    "                    \n",
    "            col_nb=randomcol\n",
    "            row_nb=randomrow\n",
    "            entire_col=get_col_value_noreplace_fromtablevalues(table_values, col_nb)\n",
    "            ctxt_in_col=[x for x in enumerate(entire_col[:row_nb]) if '[H]' in x[1] ]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            entire_row=get_row_value_noreplace_fromtablevalues(table_values, row_nb)\n",
    "            ctxt_in_row=[x for x in enumerate(entire_row[:col_nb]) if '[H]' in x[1] ]\n",
    "            \n",
    "            ctxt_txt=', '.join([x[1] for x in ctxt_in_col]+[x[1] for x in ctxt_in_row])\n",
    "            \n",
    "            if allrowspicked[elt[0]] in table_neg_dict:\n",
    "                table_neg_dict[allrowspicked[elt[0]]][allcolspicked[elt[0]]]=[value,ctxt_txt]\n",
    "            else:\n",
    "                table_neg_dict[allrowspicked[elt[0]]]=dict()\n",
    "                table_neg_dict[allrowspicked[elt[0]]][allcolspicked[elt[0]]]=[value,ctxt_txt]\n",
    "                \n",
    "               \n",
    "        \n",
    "        ######Build original table\n",
    "        table_g=[]\n",
    "        table_neg_g=[]\n",
    "        col_nb_used=list(set(col_nb_used))\n",
    "        row_nb_used=list(set(row_nb_used))\n",
    "\n",
    "        for row in sorted(row_nb_used):\n",
    "            row_g=[]\n",
    "            row_neg_g=[]\n",
    "            for col in sorted(col_nb_used):\n",
    "                if col in table_dict[row]:\n",
    "                    val=table_dict[row][col]\n",
    "                    val_n=table_neg_dict[row][col]\n",
    "                else:\n",
    "                    val=['','']\n",
    "                    val_n=['','']\n",
    "                row_g+=[val]\n",
    "                row_neg_g+=[val_n]\n",
    "            table_g+=[row_g]\n",
    "            table_neg_g+=[row_neg_g]\n",
    "            \n",
    "\n",
    "        evidence_dict=correct_ev_dict_v2(evidence_dict)\n",
    "\n",
    "        \n",
    "        if nb_gen_claim>nb_to_keep:\n",
    "            break\n",
    "        table_neg_g=remove_add_row(table_neg_g)\n",
    "        if set(evidence_dict.keys()) in all_ev_dict_keys:\n",
    "            print('ev dict already done')\n",
    "            cnt_evdone+=1\n",
    "            if cnt_evdone>25:\n",
    "                return 'NO'\n",
    "        else:\n",
    "            tables+=[{'table':table_g,'neg_table':table_neg_g,'evidence_dict':evidence_dict}] \n",
    "            all_ev_dict_keys+=[set(evidence_dict.keys())]\n",
    "            nb_gen_claim+=1\n",
    "\n",
    "        \n",
    "    return {'generated':tables,'original_one':original_one}#, 'dd':context_id_list}\n",
    "\n",
    "\n",
    "\n",
    "def gen_tables_for_t5_withneg_random_impneg_mp(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb,ns,shuffledcol):\n",
    "    ns.ret=gen_tables_for_t5_withneg_random_impneg(df, df_shuffled, cols_used,evidence_df,title,section_nb,table_nb,shuffledcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe411439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RANDOM##########################################\n",
    "elts=[]\n",
    "\n",
    "to_skip_ids=[]#54,255,277,292,343]\n",
    "to_skip=dict()\n",
    "\n",
    "for i in range(1,len(res)):\n",
    "    if i in to_skip_ids:\n",
    "        continue\n",
    "    if len(elts)>nb_seed:\n",
    "        break\n",
    "    print(i)\n",
    "\n",
    "    title=res[i]['title']\n",
    "    section_nb=res[i]['section_nb']\n",
    "    table_nb=res[i]['table_nb']\n",
    "    if not 'res_shuffled' in res[i].keys() or type(res[i]['res_shuffled'])==type(None):\n",
    "        print('No neg, skip')\n",
    "        continue\n",
    "\n",
    "    \n",
    "    new_elt=gen_tables_for_t5_withneg_random_impneg(res[i]['res'],res[i]['res_shuffled'], res[i]['cols_used'],res[i]['evidence_df'],title,section_nb,table_nb,res[i]['colshuffled'])\n",
    "\n",
    "\n",
    "    if new_elt=='NO':\n",
    "        print('skip this one, ev set identical multiples')\n",
    "        continue\n",
    "    if new_elt=='NO2':\n",
    "        print('skip this one, difficulty to replace')\n",
    "        continue\n",
    "\n",
    "    if new_elt=='NO3':\n",
    "        print('skip this one, difficulty to find cell')\n",
    "        continue\n",
    "\n",
    "\n",
    "    \n",
    "    if(len(res[i]['alternative_tables'])>0):\n",
    "        alt_broken=0\n",
    "        new_elt['tables_alternative']=[]\n",
    "        for j in range(0,len(res[i]['alternative_tables'])):\n",
    "            if alt_broken>10:\n",
    "                print('Alternative gen stopped')\n",
    "                break\n",
    "            if(j%5==0):\n",
    "                print(str(i)+','+str(j))\n",
    "            if i in to_skip.keys():\n",
    "                if j in to_skip[i] or -1 in to_skip[i] :\n",
    "                    continue\n",
    "            if nb_alt_tk<j and not nb_alt_tk==-1:\n",
    "                break\n",
    "            title=res[i]['alternative_tables'][j]['title']\n",
    "            table_nb=res[i]['alternative_tables'][j]['table_nb']\n",
    "            if type(res[i]['alternative_tables'][j]['res_shuffled'])==type(None):\n",
    "                print('No negative for ('+str(i)+','+str(j)+'), skipping')\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            mgr = multiprocessing.Manager()\n",
    "            ns = mgr.Namespace()\n",
    "            ns.ret =[]\n",
    "\n",
    "            p = multiprocessing.Process(target=gen_tables_for_t5_withnegimp_alt_random_mp, name=\"Exec\", args=(res[i]['alternative_tables'][j]['res'], res[i]['alternative_tables'][j]['res_shuffled'],res[i]['alternative_tables'][j]['cols_used'],title,table_nb,ns,res[i]['alternative_tables'][j]['colshuffled']))\n",
    "            p.start()\n",
    "            p.join(20)\n",
    "\n",
    "\n",
    "\n",
    "            if p.is_alive():\n",
    "                print(\"Generation took too long, killing\")\n",
    "                alt_broken+=1\n",
    "                # Terminate \n",
    "                p.terminate()\n",
    "                p.join()\n",
    "                continue\n",
    "            else:\n",
    "                alt_broken=0\n",
    "                new_elt_alt=ns.ret\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "            \n",
    "            new_elt['tables_alternative']+=[{'table':new_elt_alt, 'title':title,'table_nb':table_nb}]\n",
    "\n",
    "    new_elt['original_claim']=res[i]['original_claim']['claim']\n",
    "    new_elt['ctxt_nit']=res[i]['ctxt_nit']\n",
    "    new_elt['title']=res[i]['title']\n",
    "    new_elt['query']=res[i]['query']\n",
    "    new_elt['original_table']=get_table(res[i]['title'],res[i]['table_nb'])\n",
    "    new_elt['claim_type']=res[i]['claim_type']\n",
    "        \n",
    "    new_elt['original_label']=res[i]['original_label']\n",
    "    new_elt['alldiff']=res[i]['alldiff']\n",
    "    new_elt['alt_tables']=res[i]['alt_tables']\n",
    "    new_elt['nb_variables']=res[i]['nb_variables']\n",
    "    \n",
    "\n",
    "    new_elt['section']=res[i]['section']\n",
    "    if 'i' in list(res[i].keys()):\n",
    "        new_elt['i']=res[i]['i'] \n",
    "    elts+=[new_elt]\n",
    "    \n",
    "    \n",
    "\n",
    "f2=open(base_path+'/evidences_sel_g10_atleastone_240323_random.json','w')\n",
    "\n",
    "json.dump(elts,f2, indent=4)\n",
    "f2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fabf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_table_to_id(table):\n",
    "    new_table_h=[]\n",
    "    new_table_nh=[]\n",
    "    for row in enumerate(table):\n",
    "        for col in enumerate(row[1]):\n",
    "            if not '[H] ' in col[1]:\n",
    "                new_table_nh+=[[row[0],col[0]]]\n",
    "            else:\n",
    "                new_table_h+=[[row[0],col[0]]]\n",
    "    return new_table_nh, new_table_h\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_random_for_gpt(original_table, nb_ev_to_use_t='random',nb_ev_sets_to_create, technic='shuffle', title,section_nb, table_nb):\n",
    "    #technic : shuffle or generation\n",
    "    # nb ev to use random, or a nimber (likely the original number)\n",
    "    new_table_nh, new_table_h=convert_table_to_id(original_table)\n",
    "    \n",
    "    rows_with_header=sorted(list(set([x[0] for x in new_table_h])))\n",
    "    cols_with_header=sorted(list(set([x[1] for x in new_table_h])))\n",
    "    \n",
    "    tables=[]\n",
    "    ev_sets_used=[]\n",
    "    for generated_count in range(0,nb_ev_sets_to_create):\n",
    "        table_g=[]\n",
    "        if nb_ev_to_use_t=='random':\n",
    "            nb_ev_to_use=random.choice([x for x in [1,2,2,2,3,3,3,3,4,4,4,4,5,5,5,6,6,7,8] if not x>=len(new_table_nh)])\n",
    "        else:\n",
    "            nb_ev_to_use=nb_ev_to_use_t\n",
    "        \n",
    "        ev_set=[]\n",
    "        tentative=0\n",
    "        while ev_set==[] or ev_set in ev_sets_used and not tentative>30:\n",
    "            ev_set=random.sample(new_table_nh, nb_ev_to_use)\n",
    "            tentative+=1\n",
    "        if tentative>29:\n",
    "            print('problem')\n",
    "            return -1\n",
    "        table_g=[]\n",
    "        table_neg_g=[]\n",
    "        evidence_dict=dict()\n",
    "        rows_used=sorted(list(set([x[0] for x in ev_set])))\n",
    "        cols_used=sorted(list(set([x[0] for x in ev_set])))\n",
    "        \n",
    "        cols_to_fake=random.sample(cols_used,max(len(cols_used)//2,1))\n",
    "        \n",
    "        \n",
    "        headers_for_ev_set=dict()\n",
    "        for elt in ev_set:\n",
    "            \n",
    "        for row_c in rows_used:\n",
    "            row_g=[]\n",
    "            row_neg_g=[]\n",
    "            for col_c in cols_used:\n",
    "                if [row_c,col_c] in ev_set:\n",
    "                    header_col_c=[]\n",
    "                    #xid,yid,text\n",
    "                    for cell in enumerate(original_table[row_c]):\n",
    "                        if '[H]' in cell[1]:\n",
    "                            header_col_c+=[[row_c,cell[0],cell[1]]]\n",
    "                    for cell in enumerate([x[col_c] for x in original_table]):\n",
    "                        if '[H]' in cell[1]:\n",
    "                            header_col_c+=[[cell[0],col_c,cell[1]]]\n",
    "                            \n",
    "                    row_g+=[[original_table[row_c][col_c],' | '.join([x[2] for x in header_col_c])]]\n",
    "                    if col_c in cols_to_fake:\n",
    "                        errorflag=False\n",
    "                        if technic=='shuffle':\n",
    "                            new_value=None\n",
    "                            tentative=0\n",
    "                            while new_value==None and tentative<30:\n",
    "                                tentative+=1\n",
    "                                new_value=random.choice([x[col_c] for x in original_table if not(x[col_c]==original_table[row_c][col_c]) and not('[H]' in x[col_c])])\n",
    "                                new_row=[x[1] for x in enumerate(original_table[row_c]) if not x[0]==col_c else new_value]\n",
    "                                if new_row in original_table:\n",
    "                                    new_value=None\n",
    "                                    #We don't want the fake row to be in fact a true row\n",
    "                            if new_value==None:\n",
    "                                errorflag=True\n",
    "                            else:\n",
    "                                row_neg_g+=[[new_value,' | '.join([x[2] for x in header_col_c])]]\n",
    "                                \n",
    "                            \n",
    "                        if technic=='generate' or errorflag:\n",
    "                            gen_words=[]\n",
    "                            for to_test_val in [x[col_c] for x in original_table if not('[H]' in x[col_c])]:\n",
    "                                if len(gen_words)>4:\n",
    "                                    continue\n",
    "                                ####TO NOT GEN TOO MUCH USELESS FLANT5 ALTERNATIVES\n",
    "                                or_word=to_test_val.capitalize()\n",
    "                                if not or_word.replace('+','').replace('-','').replace('.','').replace(',','').isdigit():\n",
    "                                    col_i_is_int=False\n",
    "                                input_text = \"Answer the following question by giving me antonyms. Can you give me an antonym of \"+or_word+\"?\"\n",
    "                                input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "                                outputs = model.generate(input_ids)\n",
    "                                gen_word=tokenizer.decode(outputs[0]).replace('<pad> ','').replace('</s>','')\n",
    "                                if len(gen_word)>0 and not or_word.lower() in gen_word.lower(): \n",
    "                                    gen_words+=[[or_word,gen_word,len(set(or_word)-set(gen_word))+len(set(gen_word)-set(or_word))]]\n",
    "                            if len(gen_words)==0:\n",
    "                                for gen_word in ['potato','1789','Andrew']:\n",
    "                                    gen_words+=[[or_word,gen_word,len(set(or_word)-set(gen_word))+len(set(gen_word)-set(or_word))]]\n",
    "                            gen_words=sorted(gen_words,key=lambda x:x[-1])\n",
    "                            selected_replacement=gen_words[-1]\n",
    "\n",
    "                            res=selected_replacement[1]\n",
    "                            if col_i_is_int and not(res.replace('+','').replace('-','').replace('.','').replace(',','').isdigit()):\n",
    "                                res=random.randint(1,5000)\n",
    "\n",
    "                            row_neg_g+=[[res,' | '.join([x[2] for x in header_col_c])]]\n",
    "                        \n",
    "                    else:\n",
    "                        row_neg_g+=[[original_table[row_c][col_c],' | '.join([x[2] for x in header_col_c])]]\n",
    "                    evidence_dict[title+'_cell_'+str(table_nb)+'_'+str(row_c)+'_'+str(col_c)]=[title+'_title']+[title+'_header_cell_'+str(table_nb)+'_'+str(x[0])+'_'+str(x[1]) for x in header_col_c]\n",
    "                else:\n",
    "                    row_g+=[['','']]\n",
    "                    row_neg_g+=[['','']]\n",
    "                \n",
    "            table_g+=[row_g]\n",
    "            table_neg_g+=[row_neg_g]\n",
    "        tables+=[{'table':table_g,'neg_table':table_neg_g,'evidence_dict':evidence_dict}]\n",
    "    return {'generated':tables}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Used to find compatibles tables to run those queries on other datasets that don't have evidence selection (like tabfact or infotabs) data.\n",
    "\n",
    "res_headers_used=[]\n",
    "for i in range(len(res)):\n",
    "    print(i)\n",
    "    dico=dict()\n",
    "    context=res[i]['original_claim']['evidence'][0]['context']\n",
    "    query=res[i]['query']\n",
    "    claim=res[i]['original_claim']\n",
    "    all_ctxt_header=[]\n",
    "    for u in context.keys():\n",
    "        all_ctxt_header+=[x for x in context[u] if '_header_' in x]\n",
    "    okayish=True\n",
    "    for head_cell in all_ctxt_header:\n",
    "        if not head_cell.split('_')[-2]=='0':\n",
    "            okayish=False\n",
    "    if not okayish:\n",
    "        continue\n",
    "    all_ctxt_header=list(set(all_ctxt_header))\n",
    "    cells_value=get_cells_value(all_ctxt_header)\n",
    "    cells_value_cleaned=[]\n",
    "    original_table_0_dirty=get_table(res[i]['title'],res[i]['table_nb'])[0]\n",
    "    original_table_0=[]\n",
    "    \n",
    "    for elt in original_table_0_dirty:\n",
    "        if '|' in elt and ']]' in elt:\n",
    "            original_table_0+=[elt.split('|')[1].split(']]')[0]]\n",
    "        elif  ']]' in elt:\n",
    "            original_table_0+=[elt.replace('[H] ','').replace('[[','').replace(']]','')]\n",
    "        else:\n",
    "            original_table_0+=[elt.replace('[H] ','')]\n",
    "    \n",
    "    for elt in cells_value:\n",
    "        if '|' in elt and ']]' in elt:\n",
    "            cells_value_cleaned+=[elt.split('|')[1].split(']]')[0]]\n",
    "        elif  ']]' in elt:\n",
    "            cells_value_cleaned+=[elt.replace('[H] ','').replace('[[','').replace(']]','')]\n",
    "        else:\n",
    "            cells_value_cleaned+=[elt.replace('[H] ','')]\n",
    "    dico['cells_value_header']=cells_value\n",
    "    dico['all_ctxt_header']=all_ctxt_header\n",
    "    dico['original_table_0']=original_table_0\n",
    "    dico['cells_value_header_cleaned']=cells_value_cleaned\n",
    "    dico['i']=i\n",
    "    dico['cols_used']=res[i]['cols_used']\n",
    "    dico['query']=query\n",
    "    dico['claim']=claim\n",
    "    res_headers_used+=[dico] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(base_path+'/header_values.json','w')\n",
    "json.dump(res_headers_used,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d041ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_t_nb=[]\n",
    "for elt_r in res:\n",
    "    pairs_t_nb+=[[elt_r['title'],elt_r['table_nb']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68165e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(base_path+'/pair_title_tablenb.json','w')\n",
    "json.dump(pairs_t_nb,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
