{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "id": "R1U9wIQ0h6kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "7hXlXp2oiFGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "YixkKISv6c43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSCGtu1YslCq"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers==4.1.1\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import csv\n",
        "#from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    BertModel,\n",
        "    BertPreTrainedModel,\n",
        "    BertTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import ast\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUwCJfnD0r8k"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_RwiWWztpVP"
      },
      "outputs": [],
      "source": [
        "class FEVEROUSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels, use_labels = True):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.use_labels = use_labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.use_labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYjBm4qHtvFK"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    class_rep = classification_report(labels, preds, target_names= ['NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES'], output_dict=True)\n",
        "    print(class_rep)\n",
        "    print(\"Acc: {}, Recall: {}, Precision: {}, F1: {}\".format(acc, recall, precision, f1))\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'class_rep': class_rep\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf-fQ4EoLcs7"
      },
      "outputs": [],
      "source": [
        "def datasetStats(sentences, labels, useAmbLabel=False):\n",
        "  stats = {0:0, 1:0, 2:0}\n",
        "  if useAmbLabel: stats = {0:0, 1:0, 2:0, 3:0}\n",
        "  for sentence, label in zip(sentences, labels):\n",
        "    counter = stats[label]\n",
        "    counter += 1\n",
        "    stats[label] = counter\n",
        "  return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWncRwNsMqGe"
      },
      "outputs": [],
      "source": [
        "def readTSVFile(file):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  labelToUse = 0 ## CONTRADICTING / NEI\n",
        "  if 'uniform_true' in file:\n",
        "    labelToUse = 1 ## SUPPORTS\n",
        "  if 'uniform_false' in file:\n",
        "    labelToUse = 2 ## REFUTES\n",
        "  with open(file) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "      texts.append(row[0])\n",
        "      labels.append(labelToUse)\n",
        "  return texts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GREZrr2-UWOW"
      },
      "outputs": [],
      "source": [
        "def printPredictions(text_test, labels_test, predictions, labelToFilter=None):\n",
        "  for text, label, prediction in zip(text_test ,labels_test, predictions):\n",
        "    if labelToFilter is not None:\n",
        "      if label == labelToFilter: print(text, label, prediction)\n",
        "    else:\n",
        "      print(text, label, prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvW6-Guls92e"
      },
      "source": [
        "# FEVEROUS DATASET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadJsonL(fileName):\n",
        "    f = open(fileName)\n",
        "    examples = []\n",
        "    for line in f:\n",
        "        example = ast.literal_eval(line)\n",
        "        examples.append(example)\n",
        "    f.close()\n",
        "    return examples\n",
        "\n",
        "def to_sentence(selectedData):\n",
        "  text = \"\"\n",
        "  for pos in range(0, len(selectedData)):\n",
        "    text += str(selectedData[pos]).strip()\n",
        "    if pos + 1 < len(selectedData):\n",
        "      text += \" ; \"\n",
        "    else:\n",
        "      text += \".\"\n",
        "  return text.strip()\n",
        "\n",
        "def to_feverous_input(claim, evidence, evidenceContext, title):\n",
        "    sequence = []\n",
        "    sequence.append(claim)\n",
        "    sequence.append(to_sentence(list(title)))\n",
        "    sequence.append(to_sentence(evidence))\n",
        "    sequence.append(to_sentence(evidenceContext))\n",
        "    return ' </s> '.join(sequence)\n",
        "\n",
        "def toFeverousLabel(label):\n",
        "  labelToUse = 0 ## CONTRADICTING / NEI\n",
        "  if label == 'SUPPORTS':\n",
        "    labelToUse = 1 ## SUPPORTS\n",
        "  if label == 'REFUTES':\n",
        "    labelToUse = 2 ## REFUTES\n",
        "  return labelToUse\n",
        "\n",
        "def getTextLabes(loadedFile):\n",
        "  sentences = []\n",
        "  sentencesSet = set()\n",
        "  labels = []\n",
        "  for data in loadedFile:\n",
        "    if isinstance(data, dict) == False: continue\n",
        "    claim = data['claim']\n",
        "    label = data['label']\n",
        "    evidence = data['evidence']\n",
        "    evidenceContext = data['evidence_ctxt']\n",
        "    title = data['title']\n",
        "    text = to_feverous_input(claim, evidence, evidenceContext, title)\n",
        "    labelData = toFeverousLabel(label)\n",
        "    sentences.append(text)\n",
        "    sentencesSet.add(text)\n",
        "    labels.append(labelData)\n",
        "  return sentences, sentencesSet, labels\n",
        "\n",
        "BASE_PATH = \"/gdrive/MyDrive/research/tenet/data/feverousWithValues/\"  ## TODO: Change wrt to your GDRIVE Path.\n",
        "fileTrainFeverous = BASE_PATH+\"feverous_train.jsonl\"\n",
        "print(\"FILE TO OPEN:\", fileTrainFeverous)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "sentencesTrain, sentencesSetTrain, labelsTrain = getTextLabes(loadJsonL(fileTrainFeverous))\n",
        "statsTrain = datasetStats(sentencesTrain, labelsTrain)\n",
        "print(statsTrain)\n",
        "\n",
        "pairs = list(zip(sentencesTrain, labelsTrain))\n",
        "random.Random(RANDOM_SEED).shuffle(pairs)\n",
        "sentencesTrain, labelsTrain = zip(*pairs)\n",
        "sentencesTrain = list(sentencesTrain)\n",
        "labelsTrain = list(labelsTrain)\n",
        "\n",
        "fileTestFeverous = BASE_PATH+\"feverous_dev.jsonl\"\n",
        "print(\"FILE TO OPEN:\", fileTestFeverous)\n",
        "\n",
        "sentencesTest, sentencesSetTest, labelsTest = getTextLabes(loadJsonL(fileTestFeverous))\n",
        "sentencesTest.append(\"* Test Claim should be 0\")\n",
        "labelsTest.append(0)\n",
        "statsTest = datasetStats(sentencesTest, labelsTest)\n",
        "print(statsTest)\n",
        "\n",
        "EXTEND_WITH_TENET = False\n",
        "EXTEND_WITH_WARM = True\n",
        "EXTEND_WITH_COLD = False\n",
        "EXTEND_WITH_BASELINES = True\n",
        "if EXTEND_WITH_TENET:\n",
        "  if EXTEND_WITH_WARM:\n",
        "    fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_warm_50.jsonl\"\n",
        "    #fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_warm_100.jsonl\"\n",
        "    #fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_warm_200.jsonl\"\n",
        "    #fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_warm_300.jsonl\"\n",
        "    #fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_warm_400.jsonl\"\n",
        "    print(\"FILE TO OPEN:\", fileExtendTrainFeverous)\n",
        "    sentencesTrainExtend, sentencesSetTrainExtend, labelsTrainExtend = getTextLabes(loadJsonL(fileExtendTrainFeverous))\n",
        "    statsGenerated = datasetStats(sentencesTrainExtend, labelsTrainExtend)\n",
        "    print(statsGenerated)\n",
        "    sentencesTrain += sentencesTrainExtend\n",
        "    labelsTrain += labelsTrainExtend\n",
        "    pairs = list(zip(sentencesTrain, labelsTrain))\n",
        "    random.Random(RANDOM_SEED).shuffle(pairs)\n",
        "    sentencesTrain, labelsTrain = zip(*pairs)\n",
        "    sentencesTrain = list(sentencesTrain)\n",
        "    labelsTrain = list(labelsTrain)\n",
        "    print(\"Training Set Augmented\")\n",
        "    statsTrain = datasetStats(sentencesTrain, labelsTrain)\n",
        "    print(statsTrain)\n",
        "  if EXTEND_WITH_COLD:\n",
        "    #fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_cold_300.jsonl\"\n",
        "    fileExtendTrainFeverous = BASE_PATH+\"tenet_generated_cold_reversed_100.jsonl\"\n",
        "    print(\"FILE TO OPEN:\", fileExtendTrainFeverous)\n",
        "    sentencesTrainExtend, sentencesSetTrainExtend, labelsTrainExtend = getTextLabes(loadJsonL(fileExtendTrainFeverous))\n",
        "    statsGenerated = datasetStats(sentencesTrainExtend, labelsTrainExtend)\n",
        "    print(statsGenerated)\n",
        "    sentencesTrain += sentencesTrainExtend\n",
        "    labelsTrain += labelsTrainExtend\n",
        "    pairs = list(zip(sentencesTrain, labelsTrain))\n",
        "    random.Random(RANDOM_SEED).shuffle(pairs)\n",
        "    sentencesTrain, labelsTrain = zip(*pairs)\n",
        "    sentencesTrain = list(sentencesTrain)\n",
        "    labelsTrain = list(labelsTrain)\n",
        "    print(\"Training Set Augmented\")\n",
        "    statsTrain = datasetStats(sentencesTrain, labelsTrain)\n",
        "    print(statsTrain)\n",
        "\n",
        "if EXTEND_WITH_BASELINES:\n",
        "   fileExtendTrainBaseline = BASE_PATH+\"feverous_train_baseline_augmentation_not_nei.jsonl\" ## all dataset\n",
        "   print(\"FILE TO OPEN:\", fileExtendTrainBaseline)\n",
        "   sentencesTrainExtend, sentencesSetTrainExtend, labelsTrainExtend = getTextLabes(loadJsonL(fileExtendTrainBaseline))\n",
        "   statsGenerated = datasetStats(sentencesTrainExtend, labelsTrainExtend)\n",
        "   print(statsGenerated)\n",
        "   sentencesTrain += sentencesTrainExtend\n",
        "   labelsTrain += labelsTrainExtend\n",
        "   pairs = list(zip(sentencesTrain, labelsTrain))\n",
        "   random.Random(RANDOM_SEED).shuffle(pairs)\n",
        "   sentencesTrain, labelsTrain = zip(*pairs)\n",
        "   sentencesTrain = list(sentencesTrain)\n",
        "   labelsTrain = list(labelsTrain)\n",
        "   print(\"Training Set Augmented\")\n",
        "   statsTrain = datasetStats(sentencesTrain, labelsTrain)\n",
        "   print(statsTrain)"
      ],
      "metadata": {
        "id": "6Y_3ace_ME1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAIN MODEL\n",
        "\n",
        "def model_trainer(train_dataset, test_dataset, model_path, config):\n",
        "    #model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels =3, return_dict=True).to(config[\"device\"])\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\", num_labels =3, return_dict=True).to(config[\"device\"])\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_path,  # output directory\n",
        "        num_train_epochs=1,  # total # of training epochs\n",
        "        per_device_train_batch_size=4, #16,  # batch size per device during training (2 works with free plan, 4 with V100)\n",
        "        per_device_eval_batch_size=4, #16,   # batch size for evaluation (2 work with free plan, 4 with V100)\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=0,  # number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,  # strength of weight decay\n",
        "        #logging_dir=os.path.join(config[\"model_path\"], \"logs\"),  # directory for storing logs\n",
        "        logging_steps=1200,\n",
        "        save_steps=5900,  # 1200,\n",
        "        learning_rate=0.00001\n",
        "        # save_strategy='epoch'\n",
        "    )\n",
        "\n",
        "    if test_dataset != None:\n",
        "        trainer = Trainer(\n",
        "            model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
        "            args=training_args,  # training arguments, defined above\n",
        "            train_dataset=train_dataset,  # training dataset\n",
        "            eval_dataset=test_dataset,  # evaluation dataset\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "    else:\n",
        "        trainer = Trainer(\n",
        "            model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
        "            args=training_args,  # training arguments, defined above\n",
        "            train_dataset=train_dataset,  # training dataset\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "    return trainer, model\n",
        "\n",
        "MODEL_PATH_LOCAL = '/content/feverous_verdict_predictor/'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config={}\n",
        "config[\"device\"] = device\n",
        "tokenizer = RobertaTokenizer.from_pretrained('ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli')\n",
        "text_train = tokenizer(sentencesTrain, padding=True, truncation=True)\n",
        "train_dataset = FEVEROUSDataset(text_train, labelsTrain)\n",
        "text_train = tokenizer(sentencesTest, padding=True, truncation=True)\n",
        "test_dataset = FEVEROUSDataset(text_train, labelsTest)\n",
        "trainer, model = model_trainer(train_dataset, test_dataset,MODEL_PATH_LOCAL, config)\n",
        "trainer.train()\n",
        "scores = trainer.evaluate()\n",
        "print(scores[\"eval_class_rep\"])"
      ],
      "metadata": {
        "id": "kN-O92jIWAa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln5L5okY1mT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function ClickConnect(){\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "id": "nI3f97Bi3HO7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7W_gzaXj3IBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}